---
title: "Creating the ``r params$package_name`` R package"
author: "Arkajyoti Saha"
date: "September 15, 2022"
knit: litr::render
params:
  package_name: "independencepvalue" # <-- change this to your package name
  package_parent_dir: "." # <-- relative to this file's location
---

\newcommand{\bA}{{\bf A}}
\newcommand{\bX}{{\bf X}}
\newcommand{\bI}{{\bf I}}
\newcommand{\bW}{{\bf W}}
\newcommand{\bS}{{\bf S}}
\newcommand{\bT}{{\bf T}}
\newcommand{\bL}{{\bf L}}
\newcommand{\bg}{{\bf g}}
\newcommand{\bSigma}{{\boldsymbol\Sigma}}
\newcommand{\bLambda}{{\boldsymbol\Lambda}}
\newcommand{\bGamma}{{\boldsymbol\Gamma}}
\newcommand{\blambda}{\boldsymbol\lambda}
\newcommand{\P}{{\mathcal P}}
\newcommand{\hP}{\hat{\mathcal P}}

<!-- This Rmd file contains all the code needed to define an R package.  Press "Knit" in RStudio or more generally run `rmarkdown::render("create-independencepvalue.Rmd")` to generate the R package.  Remember that when you want to modify anything about the R package, you should modify this document rather than the package that is outputted.
-->

Note: This package was generated using the `litr` R package, which lets you define full R packages in a single R Markdown file. For more on `litr`, see [here](https://github.com/jacobbien/litr-project/tree/main/litr).

## Package setup

We start by specifying the information needed in the DESCRIPTION file of the R package.

```{r package-setup, message=FALSE, results='hide'}
usethis::create_package(
  path = ".",
  fields = list(
    Package = params$package_name,
    Version = "0.0.1",
    Title = "Testing independence between groups of Gaussian variables",
    Description = "Discovers block diagonal structure from a given covariance matrix through thresholding correlations and then tests whether a block is independent from the remaining variables in a fashion that accounts for the selection of the blocks.  A function for testing in the classical setting where these groups are predetermined is also implemented.",
    `Authors@R` = c(person(
      given = "Arkajyoti",
      family = "Saha",
      email = "arkajyotisaha93@gmail.com",
      role = c("aut", "cre")
      ), person("Daniela", "Witten", role="aut", email="dwitten@u.washington.edu"), person("Jacob", "Bien", role="aut", email="jbien@usc.edu"))
  )
)
usethis::use_mit_license(copyright_holder = "A. Saha")
```

## Introduction

Consider

$$
\bX\sim N_{n\times p}(0,\bI_n,\bSigma)
$$

This package implements tests of whether a particular block of the covariance matrix is zero or not, i.e.

$$
H_0^\P:\bSigma_{\P,\P^c}={\bf 0}~~\text{versus}~~H_1^\P:\bSigma_{\P,\P^c}\neq{\bf 0}.
$$
There are three main parts to the code in this package:

1. [P-values in the case of pre-specified groups:](#pre-specified) We compute the p-value under the classical context in which $\P$ is assumed to be pre-specified (i.e., not selected based on the data used for testing).  While this is not the focus of the package, some users may find this useful, and it will also be useful in the third part.

2. [Partitioning variables into groups:](#partition) Here we implement a straightforward partitioning strategy based on thresholding the correlation matrix.

3. [P-values in the case of selected groups:](#selective) This is the heart of the package.  It computes p-values in the case that $\P$ is the result of the partitioning implemented in part 2.   Since in this case the group of variables is data-dependent, we write our tested hypothesis as 
$$
H_0^{\hP(X)}:\bSigma_{\hP(X),\hP(X)^c}={\bf 0}~~\text{versus}~~H_1^{\hP(X)}:\bSigma_{\hP(X),\hP(X)^c}={\bf 0}.
$$

## Functions in the package

First, we import some required packages:

```{r, message=FALSE}
usethis::use_package("stats")
usethis::use_package("future.apply")
```

We use `future.apply` for parallelizing some of the computations.

### When groups are pre-specified {#pre-specified}

We first focus on computing the p-value when the group $\P$ is pre-specified.

First, given the group $\P$, we use (4)-(5) of the paper to compute the test statistic as a function of the canonical correlations as follows. First, we compute the cross-covariance matrix $(\bS_{\P,\P^c}^W)$ of the 'whitened' variables.

$$
\bS_{\P,\P^c}^W = \bS_{\P,\P}^{-\frac{1}{2}}\bS_{\P,\P^c}\bS_{\P^c,\P^c}^{-\frac{1}{2}}.
$$
We obtain the Singular Value Decomposition (SVD) of $\bS_{\P,\P^c}^W$ as follows:
$$\bS_{\P,\P^c}^W = \hat{\bA}\hat{\bLambda}\hat{\bGamma}^\top,$$ 
where $\hat{\bLambda} := \mathrm{diag}(\hat{\lambda}_1, \hat{\lambda}_2, \ldots, \hat{\lambda}_r)$ is the diagonal matrix of the $r = r(\P):=\min\{|\P|,|\P^c|\}$ singular values of $\bS_{\P,\P^c}^W$ and $\hat{\bA}$ and $\hat{\bGamma}$ are the left and right singular vectors of $\bS_{\P,\P^c}^W$, respectively. $\hat{\lambda}_1, \hat{\lambda}_2, \ldots, \hat{\lambda}_r$ can be interpreted as the canonical correlations between the variables in $\P$ and $\P^c$. The test statistics is given by $\prod_{i = 1}^r (1 - \hat{\lambda_i}^2)$. To compute this, we define a function that given a covariance matrix $\bS$ and a group $\P$, computes and returns the test statistic along with $\bS_{\P,\P}$, $\bS_{\P^c,\P^c}$, the left and right singular vectors of $\bS_{\P,\P^c}$, i.e. $\bS_{\P,\P}^{\frac{1}{2}}\hat{\bA}$ and $\bS_{\P^c,\P^c}^{\frac{1}{2}}\hat{\bGamma}$, since we will be using these information [when testing in the selective context](#selective).

```{r}
#' Compute the test statistic as a function of canonical correlations. 
#' 
#' Given a covariance matrix \eqn{S} and a group of variables \eqn{P}, first
#' computes the cross-covariance matrix between the whitened variables: 
#' \eqn{S_{P, P^c}^W = S_{P, P}^{-0.5} S_{P, P^c} S_{P^c, P^c}^{-0.5}}. Next, 
#' computes the SVD of \eqn{S_{P, P^c}^W} and returns the test statistic, 
#' \eqn{S_{P, P}}, \eqn{S_{P^c, P^c}} and the singular vectors 
#' of \eqn{S_{P, P^c}}.
#'
#' 
#' @param S a \eqn{p \times p} covariance matrix
#' @param CP a vector of length \eqn{p} with \eqn{i^{th}} element denoting the 
#' group \eqn{i^{th}} variable belongs to
#' @param k the group to be tested for independence with the remaining variables, i.e. \eqn{P = i : CP[i]==k}
#' @return A list containing the following items:
#' \item{statistic}{Test statistic corresponding to \eqn{S} and group of variables \eqn{P}.}
#'
#' \item{S11}{\eqn{S_{P, P}} if \eqn{2|P| \ge p}, else \eqn{S_{P^c, P^c}}.}
#'
#' \item{S22}{\eqn{S_{P^c, P^c}} if \eqn{2|P| \ge p}, else \eqn{S_{P, P}}.}
#'
#' \item{left_SV}{Left singular vectors of  \eqn{S_{P, P^c}}.}
#'
#' \item{right_SV}{Right singular vectors of \eqn{S_{P, P^c}}.}
#'
#' @keywords internal
test_stat_CCA <- function(S, CP, k) {
  p <- nrow(S)
  ptemp <- sum(CP==k)
  if(2*ptemp >= p){
    S11_x <- as.matrix(S[which(CP==k), which(CP==k)])#S11
    S22_x <- as.matrix(S[which(CP!=k), which(CP!=k)])#S22
    S12_x <- as.matrix(S[which(CP==k), which(CP!=k)])#S_12 
  }
  if(2*ptemp < p){
    S11_x <- as.matrix(S[which(CP!=k), which(CP!=k)])#S11
    S22_x <- as.matrix(S[which(CP==k), which(CP==k)])#S22
    S12_x <- as.matrix(S[which(CP!=k), which(CP==k)])#S_12 
  }
  #ensures that p2 <= p1.
  S11_x_half <- amen::mhalf(S11_x)#S_11^{1/2}
  inv_S11_x_half <- solve(S11_x_half)#S_11^{-1/2}
  S22_x_half <- amen::mhalf(S22_x)#S_11^{1/2}
  inv_S22_x_half <- solve(S22_x_half)#S_11^{-1/2}
  tilde_S12_x <- inv_S11_x_half %*% S12_x %*% inv_S22_x_half#S_12^W = covariance matrix of whitened X_1 and X_2
  svdecom <- svd(tilde_S12_x)#compact SVD 
  singular_values <- svdecom$d#lambda
  test_stat <- prod(1-singular_values^2)#test statistic 
  L_x <- S11_x_half %*% svdecom$u
  R_x <- t(svdecom$v) %*% S22_x_half
  return(list(statistic=test_stat, S11 = S11_x, S22_x = S22_x, left_SV=L_x, right_SV=t(R_x)))
}
```
We also add a code to test the function `test_stat_CCA()` works:

```{r}
testthat::test_that("test_stat_CCA() works", {
  set.seed(1)
  X <- matrix(rnorm(50), 10, 5)
  testthat::expect_equal(
    round(test_stat_CCA(S=cov(X), CP=rep(1:2, times=c(3, 2)), k=1)$statistic, 2),
    0.62)
  set.seed(1)
  X <- matrix(rnorm(50), 10, 5)
  # testing group 2 should give identical results:
  testthat::expect_equal(
    round(test_stat_CCA(S=cov(X), CP=rep(1:2, times=c(3, 2)), k=2)$statistic, 2),
    0.62)
})
```

We have used the `amen::mhalf()` for computing the symmetric square root of a matrix, so we import this package:

```{r, message=FALSE}
usethis::use_package("amen")
```

Next to compute the p-value of the test statistic, we will be sampling from the distribution of the test statistic under the null. For pre-specified groups, this is given by a Wilks' lambda distribution, and we can use Proposition 1(ii) of the paper to simulate from it as follows.

We sample two independent Wishart matrices,

$$
\bW\sim\text{Wishart}(\bI_{r},p-r)\text{ and }\bT\sim\text{Wishart}(\bI_{r},n-1-p+r),
$$

then compute the eigenvalues of $\bW\bT^{-1}$, which we call $(\Psi_1,\ldots,\Psi_r)$. Then the test statistic is given by

$$
\prod_{i=1}^r(1-\lambda_i^2)=\prod_{i=1}^r\left(1-\frac{\Psi_i}{1+\Psi_i}\right)=\frac{1}{\prod_{i=1}^r(1+\Psi_i)}.
$$

We start by defining a function that generates the random vector $(\Psi_1,\ldots,\Psi_r)$ since we'll be using this function again [when sampling in the selective context](#selective).

```{r}
#' Sample from the distribution of eigenvalues of \eqn{WT^{-1}}
#' 
#' Samples from the \code{rp}-dimensional joint distribution of the eigenvalues of 
#' \eqn{WT^{-1}}, where \eqn{W} and \eqn{T} are independent Wisharts with dimensions specified 
#' in Prop 1(ii).  These are the \eqn{\Psi_i}, and taking \eqn{\sqrt{(\Psi_i/(1+\Psi_i))}}
#' gives a sample from the joint distribution of the canonical correlations 
#' between two groups of variables of size \eqn{p_1} and \eqn{p_2} under the null.
#' 
#' @param p \eqn{p_1+p_2}
#' @param rp \eqn{min(p_1, p_2)}
#' @param n sample size
#' @return A vector of length \code{rp} sampled from the joint distribution described
#' above.
#' @keywords internal
sample_psi <- function(p, rp, n) {
  tilde_W_X <- stats::rWishart(1, p - rp, diag(rp))#simulate W
  tilde_T_X <- stats::rWishart(1, n - (p - rp) - 1, diag(rp))#simulate T
  tilde_F_X <- tilde_W_X[,,1] %*% solve(tilde_T_X[,,1])# inv(W)T
  return(eigen(tilde_F_X)$values) # eigenvalues of (inv(W)T)
}
```

```{r}
#' Function for Monte Carlo simulation for classical inference
#'
#' Samples from the joint distribution of the canonical correlations between two groups of independent Gaussian variables of size \eqn{p_1} and \eqn{p_2} using \code{sample_psi()}, and then computes the corresponding test statistic, which follows a Wilks' lambda distribution. 
#' 
#' @param p \eqn{p_1+p_2}
#' @param rp \eqn{min(p_1, p_2)}
#' @param n sample size
#' @return A sample from Wilks' lambda distribution.
#' @keywords internal
MC_function_classical <- function(p, rp, n){
  F_X_eigenvalues <- sample_psi(p, rp, n) # eigenvalues of (inv(W)T)
  while(any(F_X_eigenvalues < 0)){
    F_X_eigenvalues <- sample_psi(p, rp, n) # eigenvalues of (inv(W)T)
  }
  statistic <- 1/prod(1+F_X_eigenvalues)
  # prod(1 - lambda_i^2) = prod(1 - Psi_i/(1 + Psi_i)) = prod(1/(1 + Psi_i))
  return(statistic)
}
```

As noted in Prop 1(iii) of the paper, when $r=1$ this can be done by sampling $\lambda_1^2\sim\text{Beta}([p-1]/2, [n-p]/2)$ and then computing $(1-\lambda_1^2)$. Let's do a visual check of `MC_function_classical()` by making sure it matches up with this other approach in the $r=1$ case.

```{r}
set.seed(123)
p <- 5
n <- 20
nsim <- 1e4
from_wisharts <- sapply(1:nsim,
                        function(i) MC_function_classical(p = p, rp = 1, n = n))
from_beta <- 1 - rbeta(nsim, (p - 1) / 2, (n - p) / 2)
```

Do the quantiles from the two approaches match?

```{r}
qqplot(from_wisharts, from_beta)
abline(0, 1)
```

Yes, ok good.

Let's make a test based on the $r=1$ case:

```{r}
testthat::test_that("MC_function_classical() works", {
  set.seed(123)
  p <- 5
  n <- 20
  nsim <- 1e4
  from_wisharts <- sapply(1:nsim,
                          function(i) MC_function_classical(p = p, rp = 1, n = n))
  from_beta <- 1 - rbeta(nsim, (p - 1) / 2, (n - p) / 2)
  probs <- seq(0.05, 0.95, length = 10)
  qw <- quantile(from_wisharts,probs = probs)
  qb <- quantile(from_beta,probs = probs)
  testthat::expect_true( all(abs(qw - qb) < 0.01) )
})
```


Next, we make use of `MC_function_classical()` for a function to test the independence of two pre-specified groups of variables:

```{r}
#' Function to test the independence of two pre-specified groups of variables
#'
#' Given a covariance matrix \eqn{S} of \eqn{p} Gaussian variables, and a pre-specified group
#' of variables \eqn{P}, this function tests the null hypothesis of independence between the groups of 
#' variables in \eqn{P} and \eqn{P^c}. Makes use of \code{test_stat_CCA()} and \code{sample_psi()}.
#' 
#' @param S a \eqn{p \times p} covariance matrix
#' @param CP a vector of length \eqn{p} with \eqn{i^{th}} element denoting the 
#' group \eqn{i^{th}} variable belongs to
#' @param k the group to be tested for independence with the remaining variables, i.e. \eqn{P = [i : CP[i]==k]}
#' @param n sample size
#' @param mc_iter the number of Monte Carlo iterations to approximate the p-value
#' @return The p-value for the test of independence. 
#' @examples
#' # Simulates a 10 x 3 X_1 from N(0, I)
#' set.seed(1)
#' X_1 <- matrix(rnorm(30), 10, 3)
#'
#' # Simulates a 10 x 2 X_2 from N(0, I) independently of X_1
#' set.seed(2)
#' X_2 <- matrix(rnorm(20), 10, 2)
#'
#' # Compute the covariance matrix of X = (X_1 X_2).
#' covX <- cov(cbind(X_1, X_2))
#' # tests for a difference in means between X_1 and X_2
#' classical_p_val(S=covX, CP=rep(1:2, times=c(3, 2)), k=1, n=10, mc_iter=100)
#' @export
classical_p_val <- function(S, CP, k, n, mc_iter= 1000){
  test_hyp <- test_stat_CCA(S, CP, k)
  p <- nrow(S)
  rp <- min(p - nrow(test_hyp$S22), nrow(test_hyp$S22))
  if(rp == 1){
    classic_p_val <- 1 - stats::pbeta(1-test_hyp$statistic, (p - 3)/2 + 1, (n - p - 2)/2 + 1) 
    # for r(p) = 1, we have simple approximation of p_value based on beta distribution.
  }
  if(rp > 1){
    sip <- future.apply::future_sapply(1:mc_iter, function(i) MC_function_classical(p, rp, n), future.seed = TRUE)
    #MC simulation to approximate the p-value. 
    classic_p_val <- mean(test_hyp$statistic >= sip)#computes p-value. The test statistic is smaller if it is away from null. 
  }
  return(classic_p_val)
}
```


Next, we demonstrate an example use of the `classical_p_val()`:

```{r}
# Simulates a 10 x 3 \eqn{X_1} from \eqn{N(0, I)}
set.seed(1)
X_1 <- matrix(rnorm(30), 10, 3)

# Simulates a 10 x 2 \eqn{X_2} from \eqn{N(0, I)} independently of \eqn{X_1}
set.seed(2)
X_2 <- matrix(rnorm(20), 10, 2)
# Compute the covariance matrix of \eqn{X} = (\eqn{X_1} \eqn{X_2}).
covX <- cov(cbind(X_1, X_2))
# tests for a difference in means between \eqn{X_1} and \eqn{X_2}
classical_p_val(S=covX, n=10, CP=rep(1:2, times=c(3, 2)), k=1, mc_iter=100)
```

We also add a code to test the function `classical_p_val()`:

```{r}
testthat::test_that("classical_p_val() works", {
  set.seed(1)
  X <- matrix(rnorm(50), 10, 5)
  testthat::expect_equal(
    classical_p_val(S=cov(X), n=10, CP=rep(1:2, times=c(3, 2)), k=1, mc_iter=100),
    0.83)
  set.seed(1)
  X <- matrix(rnorm(50), 10, 5)
  # testing group 2 should give identical results:
  testthat::expect_equal(
    classical_p_val(S=cov(X), n=10, CP=rep(1:2, times=c(3, 2)), k=2, mc_iter=100),
    0.83)
})
```

### How we partition variables into groups {#partition}

Given a correlation matrix and a threshold $c$ with $0 < c< 1$, we obtain the block diagonal structure by thresholding the absolute values of the entries of the correlation matrix at c. 

```{r}
#' Function to obtain block diagonal structure through thresholding
#'
#' Given a correlation matrix \eqn{R}, this function discovers the block-diagonal structure by thresholding the absolute values of the entries of the correlation matrix at \eqn{c}. We create an adjacency matrix with the elements being 1 if and only if the corresponding member of the correlation matrix has an absolute value \eqn{\ge c}. This is equivalent to performing a single linkage hierarchical clustering on the variables, with the distance matrix given by \eqn{1 - |R|} and truncating the tree at \eqn{1-c}.
#' 
#' @param R a \eqn{p \times p} correlation matrix
#' @param c a threshold
#' @return A \eqn{p} length integer vector whose \eqn{i^{th}} element denotes the group \eqn{i^{th}} variable belongs to.
#' @examples
#' # Simulates a 10 x 5 X from N(0, I)
#' set.seed(1)
#' X <- matrix(rnorm(50), 10, 5)
#' # Compute the coreelation matrix of X.
#' corX <- cor(X)
#' # Compute the block diagonal structure at c=0.5
#' block_diag(R=corX, c=0.5)
#' @export 
block_diag <- function(R, c){
  dis_R <- 1 - abs(R)
  test <- stats::as.dist(dis_R, diag = TRUE)
  clust_result <- stats::hclust(test, method = "single")
  return(stats::cutree(clust_result, h=(1 - c)))
}
```

A test to check if `block_diag()` works:

```{r}
testthat::test_that("block_diag() works", {
  set.seed(1)
  X <- matrix(rnorm(50), 10, 5)
  testthat::expect_equal(length(unique(block_diag(cor(X), c=0.5))), 3)
})
```

### When groups are selected with `block_diag()` {#selective}

We will be using selective inference to obtain the p-value for testing the hypothesis of independence of a selected group of varirables $\hP$ (obtained using `block_diag()`) with the remaining variables.  First, we define a function to simulate from the joint distribution of the canonical correlations and check if $\hP$ is recovered by applying `block_diag()` on the perturbed covariance matrix. 

```{r}
#' Function for Monte Carlo simulation for selective inference
#'
#' Samples from the joint distirbution of the canonical correlations between two groups of independent Gaussian variables of size \eqn{p_1} and \eqn{p_2} using \code{sample_psi()}, and then computes the corresponding test statistic, which follows a Wilks' lambda distribution. Given a matrix \code{L} with \code{rp} columns, and a vector of length equal to the number of rows in \code{L}, checks if the sampled vector \eqn{\lambda} satisfies \eqn{\lambda : L\lambda \le g}
#' 
#' @param p \eqn{p_1 + p_2}
#' @param rp \eqn{min(p_1, p_2)}
#' @param n sample size
#' @param L a matrix with \code{rp} columns
#' @param g a vector of length equal to the number of rows in \code{L}
#' @return
#' \item{statistic}{Test statistic corresponding to simulated \eqn{\lambda}.}
#' \item{status}{A logical vector indicating if the simulated \eqn{\lambda} satisfies \eqn{L\lambda \leq g}.}
#' @keywords internal
MC_function_selective <- function(p, rp, n, L, g){
  if(nrow(L)!=length(g)){stop("error: number of rows of matrix L must be equal to the number of rows of vector g")}
  F_X_eigenvalues <- sample_psi(p, rp, n) # eigenvalues of (inv(W)T)
  while(any(F_X_eigenvalues<0)){
    F_X_eigenvalues <- sample_psi(p, rp, n) # eigenvalues of (inv(W)T)
  }
  statistic <- 1/prod(1+F_X_eigenvalues)
  # prod(1 - lambda_i^2) = prod(1 - Psi_i/(1 + Psi_i)) = prod(1/(1 + Psi_i))
  status <- all(L %*% sqrt(F_X_eigenvalues/(1+F_X_eigenvalues)) <= g)
  return(list(statistic=statistic, status=status))
}
```

Next, we need a function to evaluate the joint density of the canonical correlations which will be useful numerical integration based approximation of the p-value:
```{r}
#' Function to evaluate the joint density of the canonical correlations
#'
#' This function evaluates the joint density of the canonical correlations at a specific value \code{lambda}.
#' 
#' @param n sample size
#' @param p total number of variables
#' @param rp smaller of the size of the two groups of variables
#' @param a a scaling constant
#' @param lambda a \code{rp} length vector where the joint density is to be evaluated \eqn{1 \ge lambda[1]\ge lambda[2] \ge ... \ge lambda[rp] \ge 0} 
#' @return Scaled joined density of canonical correlations, evaluated at \eqn{lambda}.
#' @keywords internal
dCCA <- function(n, p, rp, a, lambda){
  c1 <- (p - 2*rp) * sum(log(lambda))
  c2 <- ((n - p - 2)/2) * sum(log(1-lambda^2))
  dip <- matrix(1, length(lambda), length(lambda))
  dip[lower.tri(dip)] <- stats::dist(lambda^2)
  c3 <- sum(log(matrixStats::colProds(dip)))
  return(exp(a+c1+c2+c3))
}
```

We have used the function `matrixStats::colProds()` in the above, so we import this package:

```{r, message=FALSE}
usethis::use_package("matrixStats")
```


Next, we make use of `MC_function_selective()` and `dCCA()` to compute the p-value for test of independence, when the group $\hP$ is obtained from `block_diag`. In order to control the selective type I error, we compute the p-value based only on the cases, where applying `block_diag()` with threshold $c$, recovers the initial grouping, observed in the data. In Theorem 2 of the paper, it was demonstrated that this is equivalent to only considering $\blambda = (\lambda_1, \lambda_2, \ldots, \lambda_r)$, which satisfy $\bL \blambda \le \bg$, where $\bL$ is a matrix (dependent on $\bS$) with $r$ columns and $\bg$ is a vector (dependent on $c$) of length $r$. For optimizing the computational overhead, we will follow three strategies depending on the values of $r$ to approximate the p-value. Recall that $r=\min\{|\P|,|\P^c|\}$.

1. $r = 1$: Here, as demonstrated in Section 4.1 of the paper, we can bypass the polytope representation and have a simple approximation in terms of ratios of cumulative beta distribution functions, given as:
$$
\hat{p}=\frac{F(g_u) - F(\min(g_u, 1 - t))}{F(g_u) - F(0)},
$$
where $g_u$ is a constant dependent on the threshold $c$ and the sample covariance matrix $\bS$ (see (S17) of Supplementary Material), $t$ is the test statistic, and $F$ is the CDF of the Beta distribution with parameters $\frac{p -1}{2}$ and $\frac{n - p}{2}$. 

2. When $r > 1$ & $r$ is small: Here, due to a lack of simple approximation, we opt for a numerical integration approach:
  * First we compute the convex hull representation of the convex polytope $\{\blambda : \bL \blambda \le \bg\}$. We use R package `rcdd` to go between different representations of polytopes.
  * Next, we use R package `volesti` to build the polytope from the convex hull and then compute the Delaunay triangulation of the convex hull with package `geometry`.
  * Now, we integrate the functions $f(\blambda)$  and $f(\blambda)\mathbb I\{\blambda:\prod_{i = 1}^r (1 - \lambda_i^2) \le \prod_{i = 1}^r (1 - \hat{\lambda_i}^2)\}$ over the simplices obtained in the previous step using package `SimplicialCubature` and take their sums. Here, $f(\cdot)$ is the joint density of \blambda (see (7) of the paper), evaluated with `dCCA()`, upto a constant. 
  * The ratio of these two quantities approximates the selective p-value. 

3. When $r$ is large: Here, for sake of reducing computational overhead, we opt of Monte Carlo approximation. We simulate $\blambda$ from their joint distribution, compute the corresponding statistic and check if they belong to the polytope $\{\blambda : \bL \blambda \le \bg\}$ with `MC_function_selective()`. 

As explained in 2., we will require some R packages. We import these packages:
```{r, message=FALSE}
usethis::use_package("volesti")
usethis::use_package("SimplicialCubature")
usethis::use_package("rcdd")
usethis::use_package("geometry")
```


```{r}
#' Function to test the independence of a data-dependent group of variables with the remaining variables
#'
#' Given a covariance matrix \eqn{S} of \eqn{p} Gaussian variables and a grouping obtained via thresholding at \eqn{c} with \code{block_diag}, this tests the null hypothesis of independence between two groups of Gaussian variables.
#' 
#' @param S a \eqn{p \times p} covariance matrix
#' @param CP a vector of length \eqn{p} with \eqn{i^{th}} element denoting the 
#' group \eqn{i^{th}} variable belongs to
#' @param k the group to be tested for independence with the remaining variables, i.e. \eqn{P = [i : CP[i]==k]}
#' @param n sample size
#' @param c a threshold
#' @param d0 a natural number; if the number of canonical correlations is greater than \code{d0}, Monte Carlo simulation will be used to approximate the p-value for computational convenience; default value is 5
#' @param mc_iter the number of Monte Carlo iterations used to approximate the p-value
#' @return The selective p-value for the test of independence.
#' @examples
#' # Simulates a 10 x 5 X from N(0, I)
#' set.seed(1)
#' X <- matrix(rnorm(50), 10, 5)
#'
#' # Compute the correlation matrix of X.
#' corX <- cor(X)
#' # Use 'block_diag' to obtain any block diagonal structure
#' block_diag_structure <- block_diag(corX, c= 0.5)
#' # test for independence of the variables in group 1 with the remaining variables
#' selective_p_val(S=cov(X), n=10, CP=block_diag_structure, c=0.5, k=1, d0=5, mc_iter=100)
#' @export 
selective_p_val <- function(S, CP, k, n, c, d0=5, mc_iter= 1000){
  test_hyp <- test_stat_CCA(S, CP, k)
  p <- nrow(S)
  p2 <- nrow(test_hyp$S22)
  p1 <- p - p2
  if(p2 == 1){
      diag_S <- diag(1/sqrt(diag(S)))
      R <- diag_S %*% S %*% diag_S
      g_u <- min(1, c^2 * test_hyp$statistic/max(abs(R[CP==k, CP!=k]))^2)
      I_denom_tot <- stats::pbeta(0, g_u, min(g_u, 1 - test_hyp$statistic), (p - 3)/2 + 1, (n - p - 2)/2 + 1)
      du <- (I_denom_tot[2] - I_denom_tot[1])/(I_denom_tot[2] - I_denom_tot[3])
  }
  else{
    L <- matrix(0, (2 * p1 * p2 + 2 *p2), p2)
    for(i in 1:p1){
      for(j in 1:p2){
        temp <- test_hyp$left_SV[i,] * test_hyp$right_SV[j,]/sqrt(test_hyp$S11[i,i]*test_hyp$S22[j, j])
        L[2*((i-1) * p2 + j)-1, ] <- temp
        L[2*((i-1) * p2 + j), ] <- - temp
      }
    }
    for(k in 1:(p2-1)){
      L[2*p1*p2 + k,k:(k+1)] <- c(-1, 1)
    }
    L[2 * p1 * p2 + p2, p2] <- -1
    for(k in 1:(p2)){
      L[2*p1*p2 + p2 + k,k] <- 1
    }
    g <- c(rep(c, 2 * p1 * p2), rep(0, p2), rep(1, p2))
    du <- 0
    if(p2 <= d0){
      P <- rcdd::makeH(L, g, x = NULL)
      PV_d <- rcdd::scdd(P)
      V_d <- as.matrix(PV_d$output[ , - c(1, 2)])
      Pi <- volesti::Vpolytope(V = V_d)
      triang = try(geometry::delaunayn(Pi@V), silent = TRUE)
      if(!inherits(triang,'try-error')){
        prod_res <- 1
        for(i in 1:p2){
          prod_res <- prod_res * gamma((n - i)/2)/(gamma((n - p2 -  i)/2) * gamma((p1 - i + 1)/2) * gamma((p2 - i + 1)/2))
        }
        alpha <- log(pi^(p2/2) * 2^p *  prod_res)
        f_tot <- function(x){
          dt <- dCCA(n, p, p2, alpha, x)
          return(c(dt, dt* (prod(1 - x^2) <= test_hyp$statistic)))
          }
        part_int <- function(i, Pi, triang, f_tot){
          if(stats::var(round(Pi@V[triang[i,], ][,1], 5)) == 0) {
            Pi@V[triang[i,], ][,1][length(Pi@V[triang[i,], ][,1])] <- Pi@V[triang[i,],][,1][length(Pi@V[triang[i,], ][,1])]*(1 - 10^(-5))
          }
          return(SimplicialCubature::adaptIntegrateSimplex(f_tot, t(Pi@V[triang[i,], ]), fDim = 2)$integral)
        }
        par_I_tot_list <- future.apply::future_sapply(1:nrow(triang), part_int, Pi, triang, f_tot, future.seed=TRUE)
        du <- sum(par_I_tot_list[2,])/sum(par_I_tot_list[1,])
        if(sum(par_I_tot_list[1,]) == 0){du <- 0}
      }
    }
    if(du <= 0 || du >= 1 || p2 > d0){
      sip <- future.apply::future_sapply(1:mc_iter, function(i) MC_function_selective(p, p2, n, L, g), future.seed=TRUE)
      zp <- sum(as.numeric(sip[2,]))
      if(zp < 100){
        sip <- future.apply::future_sapply(1:(min((mc_iter * 100/zp), 100000)), function(i) MC_function_selective(p, p2, n, L, g), future.seed=TRUE)
      }
      du <- mean(test_hyp$statistic >= sip[1,][sip[2,] == TRUE])
    }
  }
  select_p_val <- du
  return(c(select_p_val))
}
```
Next, we demonstrate an example use of the `selective_p_val()`:

```{r}
# Simulates a 10 x 5 \eqn{X_1} from \eqn{N(0, I)}
set.seed(1)
X <- matrix(rnorm(50), 10, 5)

# Compute the correlation matrix of X.
corX <- cor(X)
# Use 'block_diag()' to obtain any block diagonal structure
block_diag_structure <- block_diag(corX, c= 0.5)
# test for independence of the variables in group 1 with the remaining variables
selective_p_val(S=cov(X), CP=block_diag_structure, k=1, n=10, c=0.5, d0=5, mc_iter=100)
```

We also add a code to test the function `selective_p_val()`:

```{r}
testthat::test_that("selective_p_val() works", {
  set.seed(1)
  X <- matrix(rnorm(50), 10, 5)
  corX <- cor(cbind(X))
  block_diag_structure <- block_diag(corX, c= 0.5)
  testthat::expect_equal(round(selective_p_val(S=cov(X), n=10, CP=block_diag_structure, c=0.5, k=1, d0=5, mc_iter=100),2), 0.27)
})
```



## Documenting the package and building

We finish by running commands that will document, build, and install the package.  It may also be a good idea to check the package from within this file.

```{r}
rm(list = ls())
litr::document() # <-- use instead of devtools::document()
#devtools::build()
#devtools::install()
#devtools::check(document = FALSE)
```

