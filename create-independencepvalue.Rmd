---
title: "Creating the ``r params$package_name`` R package"
author: "Arkajyoti Saha"
date: "September 28, 2022"
knit: litr::render
params:
  package_name: "independencepvalue" # <-- change this to your package name
  package_parent_dir: "." # <-- relative to this file's location
---

\newcommand{\bA}{{\bf A}}
\newcommand{\bX}{{\bf X}}
\newcommand{\bI}{{\bf I}}
\newcommand{\bW}{{\bf W}}
\newcommand{\bS}{{\bf S}}
\newcommand{\bR}{{\bf R}}
\newcommand{\bT}{{\bf T}}
\newcommand{\bL}{{\bf L}}
\newcommand{\bg}{{\bf g}}
\newcommand{\bx}{{\bf x}}
\newcommand{\bSigma}{{\boldsymbol\Sigma}}
\newcommand{\bLambda}{{\boldsymbol\Lambda}}
\newcommand{\bGamma}{{\boldsymbol\Gamma}}
\newcommand{\blambda}{\boldsymbol\lambda}
\newcommand{\P}{{\mathcal P}}
\newcommand{\hP}{\hat{\mathcal P}}

<!-- This Rmd file contains all the code needed to define an R package.  Press "Knit" in RStudio or more generally run `rmarkdown::render("create-independencepvalue.Rmd")` to generate the R package.  Remember that when you want to modify anything about the R package, you should modify this document rather than the package that is outputted.
-->

Note: This package was generated using the `litr` R package, which lets you define full R packages in a single R Markdown file. For more on `litr`, see [here](https://github.com/jacobbien/litr-project/tree/main/litr).

## Package setup

We start by specifying the information needed in the DESCRIPTION file of the R package.

```{r package-setup, message=FALSE, results='hide'}
usethis::create_package(
  path = ".",
  fields = list(
    Package = params$package_name,
    Version = "0.0.2",
    Title = "Testing independence between groups of Gaussian variables",
    Description = "Discovers block diagonal structure from a given covariance matrix through thresholding correlations and then tests whether a block is independent from the remaining variables in a fashion that accounts for the selection of the blocks.  A function for testing in the classical setting where these groups are predetermined is also implemented.",
    `Authors@R` = c(person(
      given = "Arkajyoti",
      family = "Saha",
      email = "arkajyotisaha93@gmail.com",
      role = c("aut", "cre")
      ), person("Daniela", "Witten", role="aut", email="dwitten@u.washington.edu"), person("Jacob", "Bien", role="aut", email="jbien@usc.edu"))
  )
)
usethis::use_mit_license(copyright_holder = "A. Saha")
```

## Introduction

Consider

$$
\bX\sim N_{n\times p}(0,\bI_n,\bSigma)
$$

Given a relization $\bx$ of $\bX$, this package implements tests of whether a particular block of the covariance matrix is zero or not, i.e.

$$
H_0^\P:\bSigma_{\P,\P^c}={\bf 0}~~\text{versus}~~H_1^\P:\bSigma_{\P,\P^c}\neq{\bf 0}.
$$
There are three main parts to the code in this package:

1. [P-values in the case of pre-specified groups:](#pre-specified) We compute the p-value under the classical context in which $\P$ is assumed to be pre-specified (i.e., not selected based on the data used for testing).  While this is not the focus of the package, some users may find this useful, and it will also be useful in the third part.

2. [Partitioning variables into groups:](#partition) Here we implement a straightforward partitioning strategy based on thresholding the correlation matrix.

3. [P-values in the case of selected groups:](#selective) This is the heart of the package.  It computes p-values in the case that $\P$ is the result of the partitioning implemented in part 2.   Since in this case the group of variables is data-dependent, we write our tested hypothesis as 
$$
H_0^{\hP(\bx)}:\bSigma_{\hP(\bx),\hP(\bx)^c}={\bf 0}~~\text{versus}~~H_1^{\hP(\bx)}:\bSigma_{\hP(\bx),\hP(\bx)^c}={\bf 0}.
$$

## Functions in the package

First, we import some required packages:

```{r, message=FALSE}
usethis::use_package("stats")
usethis::use_package("future.apply")
```

We use `future.apply` for parallelizing some of the computations.

### When groups are pre-specified {#pre-specified}

We first focus on computing the p-value when the group $\P$ is pre-specified.

First, given the group $\P$, we use (4)-(5) of the paper to compute the test statistic as a function of the canonical correlations as follows. First, we compute the cross-covariance matrix $(\bS_{\P,\P^c}^W)$ of the 'whitened' variables.

$$
\bS_{\P,\P^c}^W = \bS_{\P,\P}^{-\frac{1}{2}}\bS_{\P,\P^c}\bS_{\P^c,\P^c}^{-\frac{1}{2}}.
$$
We obtain the singular value decomposition (SVD) of $\bS_{\P,\P^c}^W$ as follows:
$$\bS_{\P,\P^c}^W = \hat{\bA}\hat{\bLambda}\hat{\bGamma}^\top,$$ 
where $\hat{\bLambda} := \mathrm{diag}(\hat{\lambda}_1, \hat{\lambda}_2, \ldots, \hat{\lambda}_r)$ is the diagonal matrix of the $r = r(\P):=\min\{|\P|,|\P^c|\}$ singular values of $\bS_{\P,\P^c}^W$ and $\hat{\bA}$ and $\hat{\bGamma}$ are the left and right canonical vectors of $\bS_{\P,\P^c}^W$, respectively. $\hat{\lambda}_1, \hat{\lambda}_2, \ldots, \hat{\lambda}_r$ can be interpreted as the canonical correlations between the variables in $\P$ and $\P^c$. The test statistics is given by $\prod_{i = 1}^r (1 - \hat{\lambda_i}^2)$. To compute this, we define a function that given a covariance matrix $\bS$ and a group $\P$, computes and returns the test statistic along with $\bS_{\P,\P}$, $\bS_{\P^c,\P^c}$, the left and right singular vectors of $\bS_{\P,\P^c}$, i.e. $\bS_{\P,\P}^{\frac{1}{2}}\hat{\bA}$ and $\bS_{\P^c,\P^c}^{\frac{1}{2}}\hat{\bGamma}$, since we will be using these information [when testing in the selective context](#selective). As the test statistic is identical with $\P$ and $\P^c$ interchanged, this function will swap $\P$ and $\P^c$ if needed to ensure that $|\P|\ge p/2$. This ensures that $|\P| \ge |\P|^c$. Since $r =\min\{|\P|,|\P^c|\}$, this code ensures that $r=|\P^c|$. Moreover, for the remainder of the document, we denote $p_1:=|\P|$ and $p_2 := |\P^c| = p - p_1$.

```{r}
#' Compute the test statistic as a function of canonical correlations. 
#' 
#' Given a sample covariance matrix \eqn{S} and a group of variables \eqn{P}, first
#' computes the cross-covariance matrix between the whitened variables: 
#' \eqn{S_{P, P^c}^W = S_{P, P}^{-0.5} S_{P, P^c} S_{P^c, P^c}^{-0.5}}. Next, 
#' computes the SVD of \eqn{S_{P, P^c}^W} and returns the test statistic, 
#' \eqn{S_{P, P}}, \eqn{S_{P^c, P^c}} and the singular vectors 
#' of \eqn{S_{P, P^c}}.
#' 
#' @param S a \eqn{p \times p} sample covariance matrix
#' @param CP a vector of length \eqn{p} with \eqn{i^{th}} element denoting the 
#' group \eqn{i^{th}} variable belongs to
#' @param k the group to be tested for independence with the remaining variables, i.e. \eqn{P = i : CP[i]==k}
#' @return A list containing the following items:
#' \item{statistic}{Test statistic corresponding to \eqn{S} and group of variables \eqn{P}.}
#'
#' \item{S11}{\eqn{S_{P, P}} if \eqn{2|P| \ge p}, else \eqn{S_{P^c, P^c}}.}
#'
#' \item{S22}{\eqn{S_{P^c, P^c}} if \eqn{2|P| \ge p}, else \eqn{S_{P, P}}.}
#'
#' \item{left_SV}{Left singular vectors of  \eqn{S_{P, P^c}}.}
#'
#' \item{right_SV}{Right singular vectors of \eqn{S_{P, P^c}}.}
#'
#' @keywords internal
test_stat_CCA <- function(S, CP, k) {
  p <- nrow(S)
  ptemp <- sum(CP==k)
  if(2*ptemp >= p){
    S11_x <- as.matrix(S[which(CP==k), which(CP==k)])#S11
    S22_x <- as.matrix(S[which(CP!=k), which(CP!=k)])#S22
    S12_x <- as.matrix(S[which(CP==k), which(CP!=k)])#S_12 
  }
  if(2*ptemp < p){
    S11_x <- as.matrix(S[which(CP!=k), which(CP!=k)])#S11
    S22_x <- as.matrix(S[which(CP==k), which(CP==k)])#S22
    S12_x <- as.matrix(S[which(CP!=k), which(CP==k)])#S_12 
  }
  #ensures that p2 <= p1.
  S11_x_half <- amen::mhalf(S11_x)#S_11^{1/2}
  inv_S11_x_half <- solve(S11_x_half)#S_11^{-1/2}
  S22_x_half <- amen::mhalf(S22_x)#S_11^{1/2}
  inv_S22_x_half <- solve(S22_x_half)#S_11^{-1/2}
  tilde_S12_x <- inv_S11_x_half %*% S12_x %*% inv_S22_x_half#S_12^W = covariance matrix of whitened X_1 and X_2
  svdecom <- svd(tilde_S12_x)#compact SVD 
  singular_values <- svdecom$d#lambda
  test_stat <- prod(1-singular_values^2)#test statistic 
  L_x <- S11_x_half %*% svdecom$u
  R_x <- t(svdecom$v) %*% S22_x_half
  return(list(statistic=test_stat, S11 = S11_x, S22_x = S22_x, left_SV=L_x, right_SV=t(R_x)))
}
```
We also add a code to test the function `test_stat_CCA()` works:

```{r}
testthat::test_that("test_stat_CCA() works", {
  set.seed(1)
  X <- matrix(rnorm(50), 10, 5)
  testthat::expect_equal(
    round(test_stat_CCA(S=cov(X), CP=rep(1:2, times=c(3, 2)), k=1)$statistic, 2),
    0.62)
  set.seed(1)
  X <- matrix(rnorm(50), 10, 5)
  # testing group 2 should give identical results:
  testthat::expect_equal(
    round(test_stat_CCA(S=cov(X), CP=rep(1:2, times=c(3, 2)), k=2)$statistic, 2),
    0.62)
})
```

We have used the `amen::mhalf()` for computing the symmetric square root of a matrix, so we import this package:

```{r, message=FALSE}
usethis::use_package("amen")
```

Next to compute the p-value of the test statistic, we will be sampling from the distribution of the test statistic under the null. For pre-specified groups, this is given by a Wilks' lambda distribution, and we can use Proposition 1(ii) of the paper to simulate from it as follows.

We sample two independent Wishart matrices,

$$
\bW\sim\text{Wishart}(\bI_{r},p-r)\text{ and }\bT\sim\text{Wishart}(\bI_{r},n-1-p+r),
$$

then compute the eigenvalues of $\bW\bT^{-1}$, which we call $(\Psi_1,\ldots,\Psi_r)$. Then the test statistic is given by

$$
\prod_{i=1}^r(1-\lambda_i^2)=\prod_{i=1}^r\left(1-\frac{\Psi_i}{1+\Psi_i}\right)=\frac{1}{\prod_{i=1}^r(1+\Psi_i)}.
$$

We start by defining a function that generates the random vector $(\Psi_1,\ldots,\Psi_r)$ since we'll be using this function again [when sampling in the selective context](#selective).

```{r}
#' Sample from the distribution of eigenvalues of W * inv(T)
#' 
#' Samples from the \code{rp}-dimensional joint distribution of the eigenvalues of 
#' \eqn{WT^{-1}}, where \eqn{W} and \eqn{T} are independent Wisharts with dimensions specified 
#' in Prop 1(ii).  These are the \eqn{\Psi_i}, and taking \eqn{\sqrt{(\Psi_i/(1+\Psi_i))}}
#' gives a sample from the joint distribution of the canonical correlations 
#' between two groups of variables of size \eqn{p_1} and \eqn{p_2} under the null.
#' 
#' @param p \eqn{p_1+p_2}
#' @param rp \eqn{min(p_1, p_2)}
#' @param n sample size
#' @return A vector of length \code{rp} sampled from the joint distribution described
#' above.
#' @keywords internal
sample_psi <- function(p, rp, n) {
  tilde_W_X <- stats::rWishart(1, p - rp, diag(rp))#simulate W
  tilde_T_X <- stats::rWishart(1, n - (p - rp) - 1, diag(rp))#simulate T
  tilde_F_X <- tilde_W_X[,,1] %*% solve(tilde_T_X[,,1])# inv(W)T
  return(eigen(tilde_F_X)$values) # eigenvalues of (inv(W)T)
}
```

We use the above function in the following function, which samples from the null distribution of the test statistic:

```{r}
#' Function for Monte Carlo simulation for classical inference
#'
#' Samples from the joint distribution of the canonical correlations between two groups of independent Gaussian variables of size \eqn{p_1} and \eqn{p_2} using \code{sample_psi()}, and then computes the corresponding test statistic, which follows a Wilks' lambda distribution. 
#' 
#' @param p \eqn{p_1+p_2}
#' @param rp \eqn{min(p_1, p_2)}
#' @param n sample size
#' @return A sample from Wilks' lambda distribution.
#' @keywords internal
MC_function_classical <- function(p, rp, n){
  F_X_eigenvalues <- sample_psi(p, rp, n) # eigenvalues of (inv(W)T)
  while(any(F_X_eigenvalues < 0)){
    F_X_eigenvalues <- sample_psi(p, rp, n) # eigenvalues of (inv(W)T)
  }
  statistic <- 1/prod(1+F_X_eigenvalues)
  # prod(1 - lambda_i^2) = prod(1 - Psi_i/(1 + Psi_i)) = prod(1/(1 + Psi_i))
  return(statistic)
}
```

As noted in Prop 1(iii) of the paper, when $r=1$ this can be done by sampling 

$$
\lambda_1^2\sim\text{Beta}([p-1]/2, [n-p]/2)
$$

and then computing $(1-\lambda_1^2)$. Let's do a visual check of `MC_function_classical()` by making sure it matches up with this other approach in the $r=1$ case.

```{r}
set.seed(123)
p <- 5
n <- 20
nsim <- 1e4
from_wisharts <- sapply(1:nsim,
                        function(i) MC_function_classical(p = p, rp = 1, n = n))
from_beta <- 1 - rbeta(nsim, (p - 1) / 2, (n - p) / 2)
```

Do the quantiles from the two approaches match?

```{r}
qqplot(from_wisharts, from_beta)
abline(0, 1)
```

Yes, ok good.

Let's make a test based on the $r=1$ case:

```{r}
testthat::test_that("MC_function_classical() works", {
  set.seed(123)
  p <- 5
  n <- 20
  nsim <- 1e4
  from_wisharts <- sapply(1:nsim,
                          function(i) MC_function_classical(p = p, rp = 1, n = n))
  from_beta <- 1 - rbeta(nsim, (p - 1) / 2, (n - p) / 2)
  probs <- seq(0.05, 0.95, length = 10)
  qw <- quantile(from_wisharts,probs = probs)
  qb <- quantile(from_beta,probs = probs)
  testthat::expect_true( all(abs(qw - qb) < 0.01) )
})
```


Next, we make use of `MC_function_classical()` within a function for testing the independence of two pre-specified groups of variables.  The idea is to compute the observed value of the test statistic and then compare it to a large number of Monte Carlo samples from the null distribution of the test statistic.  When $r=1$, we do not need to use Monte Carlo sampling; rather, we use the cdf of the beta distribution described above.

```{r}
#' Function to test the independence of two pre-specified groups of variables
#'
#' Given a covariance matrix \eqn{S} of \eqn{p} Gaussian variables, and a pre-specified group
#' of variables \eqn{P}, this function tests the null hypothesis of independence between the groups of 
#' variables in \eqn{P} and \eqn{P^c}. Makes use of \code{test_stat_CCA()} and \code{sample_psi()}.
#' 
#' @param S a \eqn{p \times p} covariance matrix
#' @param CP a vector of length \eqn{p} with \eqn{i^{th}} element denoting the 
#' group \eqn{i^{th}} variable belongs to
#' @param k the group to be tested for independence with the remaining variables, i.e. \eqn{P = [i : CP[i]==k]}
#' @param n sample size
#' @param mc_iter the number of Monte Carlo iterations to approximate the p-value
#' @return The p-value for the test of independence. 
#' @examples
#' # Simulates a 10 x 3 X_1 from N(0, I)
#' set.seed(1)
#' X_1 <- matrix(rnorm(30), 10, 3)
#'
#' # Simulates a 10 x 2 X_2 from N(0, I) independently of X_1
#' set.seed(2)
#' X_2 <- matrix(rnorm(20), 10, 2)
#'
#' # Compute the covariance matrix of X = (X_1 X_2).
#' covX <- cov(cbind(X_1, X_2))
#' # tests for a difference in means between X_1 and X_2
#' classical_p_val(S=covX, CP=rep(1:2, times=c(3, 2)), k=1, n=10, mc_iter=100)
#' @export
classical_p_val <- function(S, CP, k, n, mc_iter= 1000){
  test_hyp <- test_stat_CCA(S, CP, k)
  p <- nrow(S)
  rp <- nrow(test_hyp$S22)
  if(rp == 1){
    classic_p_val <- 1 - stats::pbeta(1-test_hyp$statistic, (p - 1)/2, (n - p)/2) 
    # for r(p) = 1, we have more efficient way to compute p_value based on beta distribution.
  }
  if(rp > 1){
    sip <- future.apply::future_sapply(1:mc_iter, function(i) MC_function_classical(p, rp, n), future.seed = TRUE)
    #MC simulation to approximate the p-value. 
    classic_p_val <- mean(test_hyp$statistic >= sip)#computes p-value. The test statistic is smaller if it is away from null. 
  }
  return(classic_p_val)
}
```


Here's an example use of the `classical_p_val()`:

```{r}
# Simulates a 10 x 3 \eqn{X_1} from \eqn{N(0, I)}
set.seed(1)
X_1 <- matrix(rnorm(30), 10, 3)

# Simulates a 10 x 2 \eqn{X_2} from \eqn{N(0, I)} independently of \eqn{X_1}
set.seed(2)
X_2 <- matrix(rnorm(20), 10, 2)
# Compute the covariance matrix of \eqn{X} = (\eqn{X_1} \eqn{X_2}).
covX <- cov(cbind(X_1, X_2))
# tests for a difference in means between \eqn{X_1} and \eqn{X_2}
classical_p_val(S=covX, CP=rep(1:2, times=c(3, 2)), k=1, n=10, mc_iter=100)
```

We also add a test of the function `classical_p_val()`:

```{r}
testthat::test_that("classical_p_val() works", {
  nsim <- 1e3
  simulation_classical <- function(i){
    set.seed(i)
    X <- matrix(rnorm(50), 10, 5)
    return(classical_p_val(S=cov(X), CP=rep(1:2, times=c(3, 2)), k=1, n=10, mc_iter=100))
  }
  classical_pval <- future.apply::future_sapply(1:nsim, simulation_classical, future.seed = TRUE)
  probs <- seq(0.05, 0.95, length = 10)
  qw <- quantile(classical_pval, probs = probs)
  testthat::expect_true(all(abs(qw - probs) < 0.025) )
})
```

**ARKA: Please add a test of the `rp==1` case to ensure that this part of the code works as well.**

We add an additional test to check if the function `classical_p_val()` works in the case of $r=1$. 

```{r}
testthat::test_that("classical_p_val() works for r=1", {
  nsim <- 2e3
  simulation_classical <- function(i){
    set.seed(i)
    X <- matrix(rnorm(50), 10, 5)
    return(classical_p_val(S=cov(X), CP=rep(1:2, times=c(1, 4)), k=1, n=10, mc_iter=100))
  }
  classical_pval <- future.apply::future_sapply(1:nsim, simulation_classical, future.seed = TRUE)
  probs <- seq(0.05, 0.95, length = 10)
  qw <- quantile(classical_pval, probs = probs)
  testthat::expect_true(all(abs(qw - probs) < 0.025) )
})
```

### How we partition variables into groups {#partition}

Given a correlation matrix and a threshold $c$ with $0 < c < 1$, we obtain the block diagonal structure by thresholding the absolute values of the entries of the correlation matrix at $c$. 

```{r}
#' Function to obtain block diagonal structure through thresholding
#'
#' Given a correlation matrix \eqn{R}, this function discovers the block-diagonal structure by thresholding the absolute values of the entries of the correlation matrix at \eqn{c}. We create an adjacency matrix with the elements being 1 if and only if the corresponding member of the correlation matrix has an absolute value \eqn{\ge c}. This is equivalent to performing a single linkage hierarchical clustering on the variables, with the distance matrix given by \eqn{1 - |R|} and cutting the tree at height \eqn{1-c}.
#' 
#' @param R a \eqn{p \times p} correlation matrix
#' @param c a threshold
#' @return A \eqn{p} length integer vector whose \eqn{i^{th}} element denotes the group \eqn{i^{th}} variable belongs to.
#' @examples
#' # Simulates a 10 x 5 X from N(0, I)
#' set.seed(1)
#' X <- matrix(rnorm(50), 10, 5)
#' # Compute the correlation matrix of X.
#' corX <- cor(X)
#' # Compute the block diagonal structure at c=0.5
#' block_diag(R=corX, c=0.5)
#' @export 
block_diag <- function(R, c){
  dis_R <- 1 - abs(R)
  test <- stats::as.dist(dis_R, diag = TRUE)
  clust_result <- stats::hclust(test, method = "single")
  return(stats::cutree(clust_result, h = (1 - c)))
}
```

A test to check if `block_diag()` works:

```{r}
testthat::test_that("block_diag() works", {
  set.seed(1)
  X <- matrix(rnorm(50), 10, 5)
  testthat::expect_equal(length(unique(block_diag(cor(X), c=0.5))), 3)
})
```

### When groups are selected with `block_diag()` {#selective}

In this section we define the code for performing selective inference.  In particular, we obtain a selective p-value for testing the hypothesis of independence of a group of variables $\hP$ selected using `block_diag()` with the remaining variables.

By Theorem 1, we need to compute

$$
p(\bx,\hP)=\mathbb P(T(\blambda)\le T(\hat{\blambda}^\hP(\bx))~|~\hP\in\P(\bS'(\bx,\hP,\blambda))),
$$
where $T(\cdot)$ is the test statistic, $\P(\cdot)$ represents the group selection procedure defined by `block_diag()`, and $\blambda$ is a random vector with joint density $f$ from Proposition 1(i).

As shown in Algorithm S1 of the supplement, we evaluate this probability in one of three different ways, depending on the size of $r=\min\{|\hP|,|\hP^c|\}$:

1. $r = 1$: As demonstrated in Section 4.1 of the paper, we can express this simply in terms of the CDF of the beta distribution.

2. When $r > 1$ & $r$ is small: We use a numerical integration approach.

3. When $r$ is large: We use a Monte Carlo approximation.

Our main function for computing the selective p-value first computes the test and some other useful information by calling the function `test_stat_CCA()` and then decides which of the three tests should be performed. 

When $r > 1$ & $r$ is small (i.e. in 2.), in rare cases, the numerical integration might produce a invalid (outside $(0,1)$) approximation of the p-value (`du` in the code, please refer to `selective_p_val_integrate()` for more details on this). If `du` lies outside $(0,1)$, we approximate the p-values using Monte Carlo approximation in these cases. We initialize `du` with $0$, so $du$ lies outside $(0,1)$ by default. 

**ARKA: Add here an explanation of why `du` is set to 0 and relatedly why you check whether `du<=0`.**

```{r}
#' Test independence of a data-dependent group of variables with the rest
#'
#' Given a covariance matrix `S` of `p` Gaussian variables and a grouping obtained
#' via thresholding absolute correlations at `1-c` using `block_diag()`, this 
#' function tests the null hypothesis of independence between two groups of 
#' Gaussian variables.
#' 
#' @param S a \eqn{p \times p} covariance matrix
#' @param CP a vector of length \eqn{p} with \eqn{i^{th}} element denoting the 
#' group \eqn{i^{th}} variable belongs to
#' @param k the group to be tested for independence with the remaining variables, i.e. \eqn{P = [i : CP[i]==k]}
#' @param n sample size
#' @param c a threshold
#' @param d0 a natural number; if the number of canonical correlations is greater than \code{d0}, Monte Carlo simulation will be used to approximate the p-value for computational convenience; default value is 5
#' @param mc_iter the number of Monte Carlo iterations used to approximate the p-value
#' @return The selective p-value for the test of independence.
#' @examples
#' # Simulates a 10 x 5 X from N(0, I)
#' set.seed(1)
#' X <- matrix(rnorm(50), 10, 5)
#'
#' # Compute the correlation matrix of X.
#' corX <- cor(X)
#' # Use 'block_diag' to obtain any block diagonal structure
#' block_diag_structure <- block_diag(corX, c= 0.5)
#' # test for independence of the variables in group 1 with the remaining variables
#' selective_p_val(S=cov(X), n=10, CP=block_diag_structure, c=0.5, k=1, d0=5, mc_iter=100)
#' @export 
selective_p_val <- function(S, CP, k, n, c, d0 = 5, mc_iter = 1000){
  test_hyp <- test_stat_CCA(S, CP, k)
  p1 <- nrow(test_hyp$S11)
  p2 <- nrow(test_hyp$S22)
  if (p2 == 1 & p2 <= d0) {
    du <- selective_p_val_beta(S, CP, k, n, c, test_hyp)
  }
  else {
    du <- 0
    L <- form_L(test_hyp)
    g <- c(rep(c, 2 * p1 * p2), rep(0, p2), rep(1, p2))
    if (p2 <= d0) {
      du <- selective_p_val_integrate(n, L, g, test_hyp)
    }
    if (du <= 0 || du >= 1 || p2 > d0) {
      # use Monte Carlo approach
      du <- selective_p_val_MC(n, L, g, test_hyp, mc_iter)
    }
  }
  return(du)
}
```

We describe the functions `selective_p_val_beta()`, `selective_p_val_integrate()`, and `selective_p_val_MC()` in the next two subsections.

#### The r = 1 case

Proposition 4 in Section 4.1 of the paper shows that we can express $p(\bx,\hP)$ in terms of the CDF of the beta distribution:
$$
p(\bx,\hP)=\frac{F(g_u) - F(\min(g_u, 1 - t))}{F(g_u) - F(0)},
$$
where $t$ denotes the test statistic, $F$ is the CDF of a $\text{Beta}([p-1]/2, [n-p]/2)$ distribution, and $g_u$ is derived in (S17) of Supplementary Material to be

$$
g_u\left(\bS \left( \bx\right), \hP, c\right) := \min \left\{1, \left( \frac{c\hat\lambda_1^{\hP}\left(\bx\right)}{\max_{i' \in \hP, j' \in \hP^c}\left| \bR_{i',j'}\left(\bS\left( \bx\right)\right)\right|}\right)^2\right\}.
$$
This approach to computing the selective p-value is implemented as follows:

```{r}
#' Compute selective p-value using numerical integration
#' 
#' @param S a \eqn{p \times p} covariance matrix
#' @param CP a vector of length \eqn{p} with \eqn{i^{th}} element denoting the 
#' group \eqn{i^{th}} variable belongs to
#' @param k the group to be tested for independence with the remaining variables, i.e. \eqn{P = [i : CP[i]==k]}
#' @param n sample size
#' @param c a threshold
#' @param test_hyp output of `test_stat_CCA()`
#' 
#' @keywords internal
selective_p_val_beta <- function(S, CP, k, n, c, test_hyp) {
  p <- nrow(S)
  diag_S <- diag(1 / sqrt(diag(S)))
  R <- diag_S %*% S %*% diag_S
  g_u <-
    min(1, c ^ 2 * (1 - test_hyp$statistic) / max(abs(R[CP == k, CP != k])) ^ 2)
  I_denom_tot <- stats::pbeta(c(0, g_u, min(g_u, 1 - test_hyp$statistic)), (p - 1) / 2, (n - p) / 2)
  (I_denom_tot[2] - I_denom_tot[3])/(I_denom_tot[2] - I_denom_tot[1])
}
```

#### The r > 1 case

Theorem 2 tells us that

$$
p(\bx,\hP)=\mathbb P(T(\blambda)\le T(\hat{\blambda}^\hP(\bx))|\bL\blambda\le \bg)
$$

where $T(\cdot)$ is the test statistic, $\blambda\sim f$ from Proposition 1(i), and $\bL$ and $\bg$ depend on $\bx$, $\hP$, and $c$ (the particular functional forms of these will be given below).

The functions `selective_p_val_integrate()` and `selective_p_val_MC()` both make use of this fact, so `selective_p_val()` computes $L$ and $g$ before calling either function.

Next, we elaborate how we can derive the $\bL$ and $\bg$ used in the code, from the Supplementary Section S5 and S6 in the Supplementary Material of the article. From S6, we have for any $\left\{\lambda_i\right\}_{i= 1}^{r}$, with $1 \ge \lambda_1 \geqslant \lambda_2 \geqslant \ldots \geqslant \lambda_{r} \ge 0$, in order to recover $\hat{\mathcal P}$, with a threshold $c$ in `block_diag()` we need,

\begin{equation}
\tag{*}
\begin{pmatrix}
\mathbf{R}_{\hat{\mathcal{P}}, \hat{\mathcal{P}}^c}\left(\mathbf S'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\right) \\
-\mathbf{R}_{\hat{\mathcal{P}}, \hat{\mathcal{P}}^c}\left(\mathbf S'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\right)
\end{pmatrix} \leqslant c \mathbf{1}_{2\mathrm{card}\left(\hat{\mathcal P}\right)} \mathbf 1^\top_{\left( p - \mathrm{card}\left(\hat{\mathcal P}\right)\right)},
\end{equation}
where $\mathbf{R}\left( \mathbf S\right)$ denotes the correlation matrix corresponding to the covariance matrix $\bS$, and $\mathbf S'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)$ is a modified sample covariance matrix with the perturbed off-diagonal blocks given by 

\begin{equation}
\tag{**}
\mathbf S_{\hat{\mathcal{P}},\hat{\mathcal{P}}^c}'\left(\mathbf x, {\hat{\mathcal{P}}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right) := \mathbf U \left(\mathbf x\right) \mathrm{diag}\left(\lambda_1, \lambda_2, \ldots, \lambda_{r\left({\mathcal{R}}\right)}\right) \mathbf V \left(\mathbf x\right)^\top,
\end{equation}

where, $\mathbf U \left(\mathbf x\right)$ and $\mathbf V \left(\mathbf x\right)$ are the left and right singular vectors of $\bS_{\P,\P^c}\left(\mathbf x\right)$ respectively, i.e. $\mathbf U \left(\mathbf x\right) = \left(\bS_{\P,\P}\left(\mathbf x\right)\right)^{\frac{1}{2}}\hat{\bA}\left(\mathbf x\right)$ and $\mathbf V \left(\mathbf x\right) = \left(\bS_{\P^c,\P^c}\left(\mathbf x\right)\right)^{\frac{1}{2}}\hat{\bGamma}\left(\mathbf x\right)$. We obtain $\mathbf U \left(\mathbf x\right)$ and $\mathbf V \left(\mathbf x\right)$ directly from output of `test_stat_CCA()` (`left_SV` and `right_SV`). Using the dervation in S5, we have, 
\begin{equation}
\tag{***}
\mathbf{R}_{\hat{\mathcal{P}}, \hat{\mathcal{P}}^c}\left(\mathbf S'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\right) = \mathrm{diag}\left(\mathbf S_{\hat{\mathcal{P}}, \hat{\mathcal{P}}}'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\right)^{-\frac{1}{2}}\mathbf S_{\hat{\mathcal{P}},\hat{\mathcal{P}}^c}'\left(\mathbf x, {\hat{\mathcal{P}}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\mathrm{diag}\left(\mathbf S_{\hat{\mathcal{P}}^c, \hat{\mathcal{P}}^c}'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\right)^{-\frac{1}{2}}.
\end{equation}
Since $\mathbf S'$ only perturbs the off-diagonal component of $\bS\left( \mathbf x\right)$, we have $\mathrm{diag}\left(\mathbf S'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\right) = \mathrm{diag}\left(\mathbf S\left(\mathbf x\right)\right)$. Combining this with $(**)$ and $(***)$ we have:

$$
\mathbf{R}_{\hat{\mathcal{P}}, \hat{\mathcal{P}}^c}\left(\mathbf S'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\right) = \mathrm{diag}\left(\mathbf S_{\hat{\mathcal{P}}, \hat{\mathcal{P}}}\left(\mathbf x\right)\right)^{-\frac{1}{2}}\mathbf U \left(\mathbf x\right) \mathrm{diag}\left(\lambda_1, \lambda_2, \ldots, \lambda_{r}\right) \mathbf V \left(\mathbf x\right)^\top \mathrm{diag}\left(\mathbf S_{\hat{\mathcal{P}}^c, \hat{\mathcal{P}}^c}\left(\mathbf x\right)\right)^{-\frac{1}{2}}=\left( \left( \tilde{\mathbf U }_{i,}\odot\tilde{\mathbf V }_{j,}\blambda\right) \right)_{i,j},
$$
where $\tilde{\mathbf U } = \mathrm{diag}\left(\mathbf S_{\hat{\mathcal{P}}, \hat{\mathcal{P}}}\left(\mathbf x\right)\right)^{-\frac{1}{2}}\mathbf U \left(\mathbf x\right)$, $\tilde{\mathbf V } = \mathrm{diag}\left(\mathbf S_{\hat{\mathcal{P}}^c, \hat{\mathcal{P}}^c}\left(\mathbf x\right)\right)^{-\frac{1}{2}}\mathbf V \left(\mathbf x\right)$, $\blambda = \left(\lambda_1, \lambda_2, \ldots, \lambda_{r}\right)$, and $\mathbf a \odot \mathbf b$ denotes the elementzise product of the vectors $\mathbf a$ and $\mathbf b$. Hence elementwise product of the rows of $\tilde{\mathbf U }$ and $\tilde{\mathbf V }$ makes up the first $p_1p_2$  rows of $\bL$ correspodning to the inequalities associated with $\mathbf{R}_{\hat{\mathcal{P}}, \hat{\mathcal{P}}^c}\left(\mathbf S'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\right)$ in $(*)$. We append these $p_1p_2$ rows to $\bL$ with their signs reversed to account for the inequality corresponding to $-\mathbf{R}_{\hat{\mathcal{P}}, \hat{\mathcal{P}}^c}\left(\mathbf S'\left(\mathbf x, \hat{\mathcal{P}}, \left\{\lambda_i\right\}_{i= 1}^{r}\right)\right)$. The remaining rows of $\bL$ account for the inequalities originating from $1 \ge \lambda_1 \geqslant \lambda_2 \geqslant \ldots \geqslant \lambda_{r} \ge 0$. 
```{r}
#' Form L matrix described in Theorem 2
#' 
#' @param test_hyp output of `test_stat_CCA()`
#' 
#' @keywords internal
form_L <- function(test_hyp) {
  p1 <- nrow(test_hyp$S11)
  p2 <- nrow(test_hyp$S22)
  
  tildeU <- diag(1/sqrt(diag(test_hyp$S11))) %*% test_hyp$left_SV
  if(p2 > 1){
    tildeV <- diag(1/sqrt(diag(test_hyp$S22))) %*% test_hyp$right_SV
  }
  if(p2 == 1){
    tildeV <- 1/sqrt(diag(test_hyp$S22)) * test_hyp$right_SV
  }
  mult_list <- future.apply::future_lapply(1:p2, function(j) {res<- data.table::copy(tildeU);  as.data.frame(collapse::setop(res, "*", tildeV[j,], rowwise = T))}, future.seed = TRUE)
  L1 <- data.table::rbindlist(mult_list)
  L <- matrix(0, (2 * p1 * p2 + 2 * p2), p2)
  L[1:(2 * p1 * p2),] <- as.matrix(rbind(L1, -L1))
  if(p2 > 1){
    index <- cbind(2 * p1 * p2 + c(1:(p2 - 1)), c(1:(p2 - 1)))
    L[index] <- -1
    index[,2] <- index[,2] + 1
    L[index] <- 1
  }
  L[2 * p1 * p2 + p2, p2] <- -1
  index <- cbind(2 * p1 * p2 + p2 + c(1:(p2)), c(1:(p2)))
  L[index] <- 1
  return(L)
}
```

We have used the function `collapse::setop()` and multiple functions from the package `data.table` in the above, so we import these packages:

```{r, message=FALSE}
usethis::use_package("data.table")
usethis::use_package("collapse")
```


##### Numerical integration

The quantity $p(\bx,\hP)$ can be computed by numerical integration of the numerator and denominator of

$$
\frac{\int_{\left\{\bL\blambda \leq \bg\right\}} f\left(\blambda\right)g\left(\blambda\right)d\lambda_1d\lambda_2\ldots d\lambda_{r\left(\hP\right)}}{\int_{\bL\blambda \leq \bg} f\left(\blambda\right)d\lambda_1d\lambda_2\ldots d\lambda_{r\left(\hP\right)}}
$$

where

$$
g\left(\blambda\right)=1\left\{\prod_{i= 1}^{r\left(\hP\right)}  \left(1 - \lambda_i^2\right) \leq  \prod_{i = 1}^{r\left(\hP\right)} \left(1 - \left(\hat\lambda_i^{\hP}\left(\bx\right)\right)^2\right)\right\}
$$

and $f$ is the joint density of $\blambda$ under the null.  The following function evaluates $f$ (up to a scaling):

```{r}
#' Function to evaluate the joint density of the canonical correlations
#'
#' This function evaluates the joint density of the canonical correlations at a specific value \code{lambda}.
#' 
#' @param n sample size
#' @param p total number of variables
#' @param rp smaller of the size of the two groups of variables
#' @param a a scaling constant
#' @param lambda a \code{rp} length vector where the joint density is to be evaluated \eqn{1 \ge lambda[1]\ge lambda[2] \ge ... \ge lambda[rp] \ge 0} 
#' @return Scaled joined density of canonical correlations, evaluated at \eqn{lambda}.
#' @keywords internal
dCCA <- function(n, p, rp, a, lambda){
  c1 <- (p - 2*rp) * sum(log(lambda))
  c2 <- ((n - p - 2)/2) * sum(log(1-lambda^2))
  dip <- matrix(1, length(lambda), length(lambda))
  dip[lower.tri(dip)] <- stats::dist(lambda^2)
  c3 <- sum(log(dip))
  return(exp(a+c1+c2+c3))
}
```


The function `selective_p_val_integrate()` relies on efficient code for performing numerical integration over simplices.  In particular, it does the following:
  * First we compute the convex hull representation of the convex polytope $\{\blambda : \bL \blambda \le \bg\}$. We use the R package `rcdd` to go between different representations of polytopes.
  * Next, we use the R package `volesti` to build the polytope from the convex hull and then compute the Delaunay triangulation of the convex hull with the package `geometry`.
  * Next, we integrate the functions $f(\blambda)$  and $f(\blambda)g(\blambda)$ over the simplices obtained in the previous step using the package `SimplicialCubature` and take their sums. Here, $f(\cdot)$ is the joint density of $\blambda$ (see (7) of the paper), evaluated with `dCCA()`, up to a constant. 
  * The ratio of these two quantities approximates the selective p-value. 

We start by importing these packages:

```{r, message=FALSE}
usethis::use_package("volesti")
usethis::use_package("SimplicialCubature")
usethis::use_package("rcdd")
usethis::use_package("geometry")
```


Due to numerical instability, in rare cases, this numerical integration based approximation of p-value (`du` in the code) may produce an invalid estimates (`NA`). We force `selective_p_val_integrate()` to return $0$ in these cases and use the Monte Carlo simulation based approximation in `selective_p_val_MC()` instead. For more details please refer to `selective_p_val()`. Below we describe the ways this numerical integration may lead to invalid results:

1. The numerical integration depends on Delaunay triangulation. In very rare cases, this might fail due to boundary cases. We initialize `du` with $0$ and catch the failure in obtaining Delaunay triangulation with `try()`. We only proceed with the numerical integration, if the Delaunay triangulation is successful. This ensures that we return $0$ when the triangulation fails. 

2. Due to numerical instability, the integral in the denominator might turn out to be very close to $0$ (negligible with respect to default machine precision), especially when the true value of the denominator is very small. In those cases, to avoid returning `NA`, we force `du` to be $0$.

Since we consider the ratio of sum of the integrals, the constant part (`alpha` in the code) cancels out from both the numerator and denominator. We keep the `alpha` in the code to ensure that the integral in the denominator doesn't become too small, especially when the true value of the denominator is very small. 

```{r}
#' Compute selective p-value using numerical integration
#' 
#' @param n sample size
#' @param L matrix used to define conditioning set
#' @param g vector used to define conditioning set
#' @param test_hyp output of `test_stat_CCA()`
#' 
#' @keywords internal
selective_p_val_integrate <- function(n, L, g, test_hyp) {
  p1 <- nrow(test_hyp$S11)
  p2 <- nrow(test_hyp$S22)
  p <- p1 + p2

  du <- 0 # initialize du
  P <- rcdd::makeH(L, g, x = NULL) # create the Half space representation of the polytope
  PV_d <- rcdd::scdd(P) # compute the convex hull representation of the polytope.
  V_d <- as.matrix(PV_d$output[,-c(1, 2)])
  Pi <- volesti::Vpolytope(V = V_d) # create a convenient Vertex representation from the convex hull. 
  triang = try(geometry::delaunayn(Pi@V), silent = TRUE) # try the Delaunay triangulation
  if (!inherits(triang, 'try-error')) { # if the triangulation is successful
    prod_res <- 1
    for (i in 1:p2) {
      prod_res <-
        prod_res * gamma((n - i) / 2) / (gamma((n - p2 -  i) / 2) * gamma((p1 - i + 1) /
                                                                            2) * gamma((p2 - i + 1) / 2))
    }
    alpha <- log(pi ^ (p2 / 2) * 2 ^ p *  prod_res) # compute the constant
    f_tot <- function(x) {
      dt <- dCCA(n, p, p2, alpha, x)
      return(c(dt, dt * (prod(1 - x ^ 2) <= test_hyp$statistic))) # compute the function in the numerator
    }
    part_int <- function(i, Pi, triang, f_tot) { #code to perform integration
      if (stats::var(round(Pi@V[triang[i, ],][, 1], 5)) == 0) {
        Pi@V[triang[i, ],][, 1][length(Pi@V[triang[i, ],][, 1])] <-
          Pi@V[triang[i, ], ][, 1][length(Pi@V[triang[i, ],][, 1])] * (1 - 10 ^ (-5)) # Account for numerical instabilities on the triangulation by adding negligble deviation
      }
      return(SimplicialCubature::adaptIntegrateSimplex(f_tot, t(Pi@V[triang[i, ],]), fDim = 2)$integral) # perform numerical integration on the simplices
    }
    par_I_tot_list <-
      future.apply::future_sapply(1:nrow(triang), part_int, Pi, triang, f_tot, future.seed =
                                    TRUE)
    du <- sum(par_I_tot_list[2, ]) / sum(par_I_tot_list[1, ])
    if (sum(par_I_tot_list[1, ]) == 0) { # if the integral in the denominator is effectively zero, set du to 0.
      du <- 0
    }
  }
  return(du)
}
```


##### Monte Carlo approach

Here we define the Monte Carlo approach to approximating

$$
p(\bx,\hP)=\mathbb P(T(\blambda)\le T(\hat{\blambda}^\hP(\bx))|\bL\blambda\le \bg).
$$

The function `sample_psi()` (that we used in the classical p-value case) allows us to sample from $\blambda\sim f$.  The Monte Carlo approach will sample many such $\blambda$ vectors and then keep those for which $\bL\blambda\le \bg$.  This corresponds to sampling from the conditional distribution.  We then will compare the actual test statistic to the samples from the conditional distribution.

The following function draws a random vector $\blambda$ from the null distribution, computes the corresponding statistic, and then checks if the drawn $\blambda$ belongs to the polytope $\{\blambda : \bL \blambda \le \bg\}$.


```{r}
#' Function for Monte Carlo simulation for selective inference
#'
#' Samples from the joint distribution of the canonical correlations between two groups of independent Gaussian variables of size \eqn{p_1} and \eqn{p_2} using \code{sample_psi()}, and then computes the corresponding test statistic, which follows a Wilks' lambda distribution. Given a matrix \code{L} with \code{rp} columns, and a vector of length equal to the number of rows in \code{L}, checks if the sampled vector \eqn{\lambda} satisfies \eqn{\lambda : L\lambda \le g}
#' 
#' @param p \eqn{p_1 + p_2}
#' @param rp \eqn{min(p_1, p_2)}
#' @param n sample size
#' @param L a matrix with \code{rp} columns
#' @param g a vector of length equal to the number of rows in \code{L}
#' @return
#' \item{statistic}{Test statistic corresponding to simulated \eqn{\lambda}.}
#' \item{status}{A logical vector indicating if the simulated \eqn{\lambda} satisfies \eqn{L\lambda \leq g}.}
#' @keywords internal
MC_function_selective <- function(p, rp, n, L, g){
  if(nrow(L)!=length(g)){stop("error: number of rows of matrix L must be equal to the number of rows of vector g")}
  F_X_eigenvalues <- sample_psi(p, rp, n) # eigenvalues of (inv(W)T)
  while(any(F_X_eigenvalues<0)){
    F_X_eigenvalues <- sample_psi(p, rp, n) # eigenvalues of (inv(W)T)
  }
  statistic <- 1/prod(1+F_X_eigenvalues)
  # prod(1 - lambda_i^2) = prod(1 - Psi_i/(1 + Psi_i)) = prod(1/(1 + Psi_i))
  status <- all(L %*% sqrt(F_X_eigenvalues/(1+F_X_eigenvalues)) <= g)
  return(list(statistic=statistic, status=status))
}
```

This function is then repeatedly called to form a Monte Carlo estimate of the selective p-value.

```{r}
#' Compute selective p-value using Monte Carlo
#' 
#' @param n sample size
#' @param L matrix used to define conditioning set
#' @param g vector used to define conditioning set
#' @param test_hyp output of `test_stat_CCA()`
#' @param mc_iter number of Monte Carlo iterations
#' 
#' @keywords internal
selective_p_val_MC <- function(n, L, g, test_hyp, mc_iter) {
  p1 <- nrow(test_hyp$S11)
  p2 <- nrow(test_hyp$S22)
  p <- p1 + p2
  sip <-
    future.apply::future_sapply(1:mc_iter, function(i)
      MC_function_selective(p, p2, n, L, g), future.seed = TRUE)
  zp <- sum(as.numeric(sip[2, ]))
  if (zp < 100) {
    sip <-
      future.apply::future_sapply(1:(min((mc_iter * 100 / zp), 100000)), function(i)
        MC_function_selective(p, p2, n, L, g), future.seed = TRUE)
  }
  mean(test_hyp$statistic >= sip[1, ][sip[2, ] == TRUE])
}
```

#### Testing `selective_p_val()`

Next, we demonstrate an example use of the `selective_p_val()`:

```{r}
# Simulates a 10 x 5 \eqn{X_1} from \eqn{N(0, I)}
set.seed(4)
X <- matrix(rnorm(600), 30, 20)
# Compute the correlation matrix of X.
corX <- cor(X)
# Use 'block_diag()' to obtain any block diagonal structure
block_diag_structure <- block_diag(corX, c=0.3)
# test for independence of the variables in group 3 with the remaining variables
set.seed(1)
selective_p_val(S=cov(X), CP=block_diag_structure, k=6, n=30, c=0.3, d0=5, mc_iter=100)
```

We also add a code to test the function `selective_p_val()`:
```{r}
testthat::test_that("selective_p_val() works", {
nsim <- 1e3
  simulation_selective <- function(i){
    set.seed(i)
    X <- matrix(rnorm(50), 10, 5)
    corX <- cor(X)
    block_diag_structure <- block_diag(corX, c=0.5)
    if(length(unique(block_diag_structure)) > 1){
      set.seed(i)
      k0 <- sample(unique(block_diag_structure), 1)
      return(selective_p_val(S=cov(X), CP=block_diag_structure, k=k0, n=10, c=0.5, d0=2, mc_iter=100))
    }
  }
  selective_pval <- sapply(1:nsim, simulation_selective)
  probs <- seq(0.05, 0.95, length = 10)
  qw <- quantile(unlist(selective_pval), probs = probs)
  testthat::expect_true(all(abs(qw - probs) < 0.025) )
})
```

We add a test to check if `selective_p_val_beta()` and `selective_p_val_MC()` produce identical results (within an acceptable tolerance) for $r=1$.
```{r}
testthat::test_that("selective_p_val_beta() and selective_p_val_MC() produce similar results", {
set.seed(4)
X <- matrix(rnorm(600), 30, 20)
corX <- cor(cbind(X))
block_diag_structure <- block_diag(corX, c=0.3)
table(block_diag_structure)
# We test group 6 as r = 1 here. Setting d0 >= 1 will make selective_p_val() use selective_p_val_beta() for approximation
set.seed(1)
p_beta <- selective_p_val(S=cov(X), CP=block_diag_structure, k=6, n=30, c=0.3, d0=5, mc_iter=1000)
set.seed(1)
# Setting d0 = 0 forces selective_p_val() to use selective_p_val_MC() for approximation, even if r = 1
p_MC <- selective_p_val(S=cov(X), CP=block_diag_structure, k=6, n=30, c=0.3, d0=0, mc_iter=1000)
testthat::expect_true(abs(p_beta - p_MC) < 0.025)
})
```


We add a test to check if `selective_p_val_integrate()` and `selective_p_val_MC()` produce identical results (within an acceptable tolerance) for small values of $r$.
```{r}
testthat::test_that("selective_p_val_integrate() and selective_p_val_MC() produce similar results", {
set.seed(4)
X <- matrix(rnorm(600), 30, 20)
corX <- cor(cbind(X))
block_diag_structure <- block_diag(corX, c=0.3)
table(block_diag_structure)
# We test group 2 as r = 2 here. Setting d0 >= 2 will make selective_p_val() use selective_p_val_integrate() for approximation. 
set.seed(1)
p_integrate <- selective_p_val(S=cov(X), CP=block_diag_structure, k=2, n=30, c=0.3, d0=5, mc_iter=1000)
set.seed(1)
# Setting d0 = 0 forces selective_p_val() to use selective_p_val_MC() for approximation, even if r = 1
p_MC <- selective_p_val(S=cov(X), CP=block_diag_structure, k=2, n=30, c=0.3, d0=0, mc_iter=1000)
testthat::expect_true(abs(p_integrate - p_MC) < 0.025)
})
```


**ARKA: Please make sure your tests cover (at least) the three cases of r=1, r small, and r large.**

## Documenting the package and building

We finish by running commands that will document, build, and install the package.  It may also be a good idea to check the package from within this file.

```{r}
rm(list = ls())
litr::document() # <-- use instead of devtools::document()
#devtools::build()
#devtools::install()
#devtools::check(document = FALSE)
```

