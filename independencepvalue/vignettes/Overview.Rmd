---
title: "independencepvalue: Testing independence between data-driven groups of Gaussian variables"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{independencepvalue_overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
# What does `independencepvalue` do?
`independencepvalue` is an `R` package that tests the independence between two groups of Gaussian variables, where the groups were obtained by thresholding the correlation matrix.

# How do I get `independencepvalue`?

We use `R` package `remotes` to install `independencepvalue` from GitHub. If not installed, please install `remotes` by using `install.packages("remotes")`. Next run,

```{r setup, eval=FALSE}
remotes::install_github("ArkajyotiSaha/independencepvalue-project", subdir = "independencepvalue")
library(independencepvalue)
```

This R package is created using literate programming with the [litr](https://github.com/jacobbien/litr-project/tree/main/litr) R package. To know more about `independencepvalue` generation, please visit the [package repository](https://github.com/ArkajyotiSaha/independencepvalue-project).

# Why is `independencepvalue` needed?

```{r classical, echo = FALSE}
p <- 6
n <- 9
a <- 0.6
b <- 0.3

Sigma_11 <- QRM::equicorr(p/2, a)
Sigma_22 <- QRM::equicorr(p/2, a)

Sigma <- as.matrix(Matrix::bdiag(Sigma_11, Sigma_22))
Sigma[((p/2+1):p), (1:p/2)] <- b
Sigma[(1:p/2), ((p/2+1):p)] <- b

i0 <- 9768
set.seed(i0)
X <- MASS::mvrnorm(n=n, rep(0, p), Sigma)
block_diag_structure <- independencepvalue::block_diag(cor(X), c=0.5)
k1 <- 1
```

In theory, one should decide the hypotheses to test before seeing the data. However, in practice, often scientists want to find something seemingly interesting in the data and then use the data to test if it is really interesting. When we generate a hypothesis based on a data set, and then use the same data set to test the hypothesis, classical hypothesis testing may lead to invalid results. To illustrate: we consider a simulated dataset, where the variables are correlated (as shown in the heatmap of the absolute entries of the absolute and sample correlation matrix). In this dataset, we first identify groups of seemingly uncorrelated variables via thresholding the correlation matrix. We want to test the null hypothesis that they are really uncorrelated. A classical approach based on Wilks' lambda distribution yielded a p-value of  `r set.seed(i0)
independencepvalue::classical_p_val(S=cov(X), CP=block_diag_structure, k=k1, n=n, mc_iter=1000)`, even when the variables are correlated in the population! This shows how ignoring the fact that the hypothesis was selected from the data can lead to a tremendous loss in power.

```{r plot, echo = FALSE, fig.height = 5, fig.width = 8, fig.align = "center", dpi = 100}
library(plot.matrix)
par(fig=c(0.01, 0.5, 0, 0.8))
plot(Sigma, breaks=c(0, 1), main="(a) Absolute population correlation", xlab=NA, ylab=NA, col=rev(heat.colors(10)), key=NULL)
par(fig=c(0.51, 1, 0, 0.8),new=TRUE)
plot(abs(cor(X)), breaks=c(0, 1), main="(b) Absolute sample correlation", xlab=NA, ylab=NA, col=rev(heat.colors(10)), key=NULL)
par(fig=c(0, 1, 0, 1),new=TRUE)
rect(
  head(seq(0.85, 5.85, 5/10), -1),
  6.75,
  tail(seq(0.85, 5.85, 5/10), -1),
  7,
  col=rev(heat.colors(10))
)
mtext((1:10)/10, side=3, at=tail(seq(0.85, 5.85, 5/10), -1)-0.25)
```

This is where `independencepvalue` comes into play. It accounts for the fact that the hypothesis was generated by thresholding the correlation matrix of the data. Applying `independencepvalue` in the aforementioned scenario leads to a p-value of `r set.seed(i0)
round(independencepvalue::selective_p_val(S=cov(X), CP=block_diag_structure, k=k1, n=n, c=0.5, d0=5, mc_iter= 1000), 3)`. Unlike the classical inference, this test correctly identifies the group of variables to be correlated. To know more about how to use `independencepvalue`, please see the tutorial. 

# How does `independencepvalue` work?
`independencepvalue` works in two stages:

 1. First, we threshold the absolute values of the observed correlation matrix to partition the variables into groups of seemingly uncorrelated variables. 
 2. Next, we take a selective inference approach to test if a group of variables (obtained in the previous step) is independent of the remaining variables. Here, unlike the classical approach, we do not ignore that the hypothesis was generated from the data. We account for this by conditioning on the event that the grouping of the variables that generated the hypothesis, is recovered. We adjust the null distribution to account for the selection of the particular hypothesis being tested. By restricting attention to specifically those datasets that would have led to this hypothesis being selected, we are able to account for the selection effect.

For more details, please see our paper, [Inferring independent sets of Gaussian variables after thresholding correlations](XXXX.arxiv.org).



